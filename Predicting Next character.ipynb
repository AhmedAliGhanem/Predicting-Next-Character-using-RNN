{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import SimpleRNN, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv1D, MaxPooling1D, ZeroPadding1D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "import cPickle as pickle\n",
    "import bcolz\n",
    "import re\n",
    "from numpy.random import random, permutation, randn, normal, uniform, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600901\n"
     ]
    }
   ],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()\n",
    "print len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a vocabulary of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print len(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting 0 as it wasn't in the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary, mapping characters to index and index to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_to_index = {v:i for i,v in enumerate(chars)}\n",
    "index_to_char = {i:v for i,v in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the entire nietzsche text into index of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_index = [char_to_index[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(index_to_char[i] for i in total_index[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As were are predicting the 8th character, we need to create an array of the first 7 characters each acting as an input and the last character as the output.\n",
    "\n",
    "For example, for the text 'this and that'\n",
    "\n",
    "The input will be -> [['t', ' '], ['h', 't'], ['i', 'h'], ['s', 'a'], [' ', 't'], ['a'], ['n']] -> but instead of the characters, there will be the index of the character.\n",
    "\n",
    "And the output will be -> ['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_num = 7\n",
    "xin = [[total_index[j+i] for j in xrange(0, len(total_index)-1-pred_num, pred_num)] for i in range(pred_num)]\n",
    "y = [total_index[i+pred_num] for i in xrange(0, len(total_index)-1-pred_num, pred_num)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing the last 2 characters to keep the length of each array equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [np.stack(xin[i][:-2]) for i in range(pred_num)]\n",
    "Y = np.stack(y[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 39, ..., 54, 57, 58]),\n",
       " array([42,  1, 43, ..., 67,  2,  2]),\n",
       " array([29,  1, 33, ...,  2, 54, 62]),\n",
       " array([30, 43, 38, ..., 76, 72, 67]),\n",
       " array([25, 45, 31, ..., 68,  2, 57]),\n",
       " array([27, 40,  2, ..., 71, 73, 62]),\n",
       " array([29, 40, 73, ..., 65, 61, 56])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 39, 61, 73,  2,  9, 61,  2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((85840,), (85840,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layers = 256\n",
    "vocab_size = 86\n",
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=pred_num),\n",
    "        SimpleRNN(hidden_layers, activation='relu'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 7, 42)             3612      \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 256)               76544     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 86)                22102     \n",
      "=================================================================\n",
      "Total params: 102,258.0\n",
      "Trainable params: 102,258\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(np.stack(X, 1), Y, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('simpleRNN_3pred.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('simpleRNN_3pred.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('simpleRNN_7pred.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('simpleRNN_7pred.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First convert the input to indices\n",
    "2. Then expand the dimension to match the model's output format\n",
    "3. Predict the 8th character using the input\n",
    "4. As we are using softmax activation in the last layer of the model, we get the probability of every 86 characters in our vocabulary. So the character with the maximum probability will be the 8th predicted character by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_next_char(inp):\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(np.array(index), axis=0)\n",
    "    prediction = model.predict(arr)\n",
    "    return index_to_char[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, prediction is being done for the 8th character(pred_num = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char('those w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, prediction is being done for the 4th character, so just set pred_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char(' an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_char('does th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Return Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will predict the next word where the input will be all the words before it.\n",
    "\n",
    "For example, to predict the 2nd word, first word will be used\n",
    "\n",
    "To predict the 3rd word, first and second word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [[total_index[j+i] for j in xrange(1, len(total_index)-pred_num, pred_num)] for i in range(pred_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_return = [np.stack(ys[i][:-2]) for i in range(pred_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40,  1, 39, ..., 54, 57, 58]),\n",
       " array([42,  1, 43, ..., 67,  2,  2]),\n",
       " array([29,  1, 33, ...,  2, 54, 62]),\n",
       " array([30, 43, 38, ..., 76, 72, 67]),\n",
       " array([25, 45, 31, ..., 68,  2, 57]),\n",
       " array([27, 40,  2, ..., 71, 73, 62]),\n",
       " array([29, 40, 73, ..., 65, 61, 56])]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([42,  1, 43, ..., 67,  2,  2]),\n",
       " array([29,  1, 33, ...,  2, 54, 62]),\n",
       " array([30, 43, 38, ..., 76, 72, 67]),\n",
       " array([25, 45, 31, ..., 68,  2, 57]),\n",
       " array([27, 40,  2, ..., 71, 73, 62]),\n",
       " array([29, 40, 73, ..., 65, 61, 56]),\n",
       " array([ 1, 39, 61, ..., 57, 58, 54])]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 86\n",
    "n_fac = 42\n",
    "hidden_layers = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are setting return_sequences=True, we need to wrap the Dense layer in a TimeDistributed Layer since it is a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return_model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=pred_num),\n",
    "        SimpleRNN(hidden_layers, return_sequences=True, activation='relu'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 7, 42)             3612      \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 7, 256)            76544     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 7, 86)             22102     \n",
      "=================================================================\n",
      "Total params: 102,258.0\n",
      "Trainable params: 102,258\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "return_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_model = np.stack(X, 1)\n",
    "Y_model = np.expand_dims(np.stack(Y_return, 1), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "85840/85840 [==============================] - 36s - loss: 2.3465    \n",
      "Epoch 2/5\n",
      "85840/85840 [==============================] - 36s - loss: 1.9845    \n",
      "Epoch 3/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.8886    \n",
      "Epoch 4/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.8391    \n",
      "Epoch 5/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.8063    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7c09d3890>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_model.fit(X_model, Y_model, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "85840/85840 [==============================] - 36s - loss: 1.7839    \n",
      "Epoch 2/5\n",
      "85840/85840 [==============================] - 36s - loss: 1.7660    \n",
      "Epoch 3/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.7523    \n",
      "Epoch 4/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.7410    \n",
      "Epoch 5/5\n",
      "85840/85840 [==============================] - 37s - loss: 1.7311    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7cd152f90>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_model.optimizer.lr = 1e-4\n",
    "return_model.fit(X_model, Y_model, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_every_char(inp):\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(index, axis=0)\n",
    "    prediction = return_model.predict(arr)\n",
    "    return [index_to_char[np.argmax(i)] for i in prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'd', ' ', 't', 'h', 'e', ' ']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_every_char('and the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'e', 'n', ' ', 'o', 'n', ' ']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_every_char('this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stateful model, the model remembers the context i.e. the long-term dependencies. Make sure you set shuffle=False.\n",
    "\n",
    "Because if you set shuffle=True, the order of input will not be preserved, hence the model won't be able to extract the context of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use ReLU as the activation of LSTM, the loss shoots up to infinity and then NaN.\n",
    "\n",
    "Hence using tanh as the activation keeps the hidden state vector from growing beyond [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stateful_model = Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=pred_num, batch_input_shape=(bs, 7)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(hidden_layers, activation='tanh', return_sequences=True, stateful=True),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stateful_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "divide = len(X_model)//bs*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "85824/85824 [==============================] - 123s - loss: 2.2646   \n",
      "Epoch 2/5\n",
      "85824/85824 [==============================] - 124s - loss: 2.0132   \n",
      "Epoch 3/5\n",
      "85824/85824 [==============================] - 124s - loss: 1.9413   \n",
      "Epoch 4/5\n",
      "85824/85824 [==============================] - 117s - loss: 1.8996   \n",
      "Epoch 5/5\n",
      "85824/85824 [==============================] - 109s - loss: 1.8699   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7b4484590>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateful_model.fit(X_model[:divide], Y_model[:divide], batch_size=64, epochs=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_every_char_stateful(inp):\n",
    "    index = [char_to_index[i] for i in inp]\n",
    "    arr = np.expand_dims(index, axis=0)\n",
    "    prediction = return_model.predict(arr)\n",
    "    return [index_to_char[np.argmax(i)] for i in prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'e', 'n', ' ', 'o', 'n', ' ']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_every_char_stateful('this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'n', 'd', ' ', 't', 'o', 'r']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_every_char_stateful(' and fo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
